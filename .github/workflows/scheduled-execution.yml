name: 'Scheduled Lambda Execution'

on:
  schedule:
    # Run daily at 06:00 UTC (adjust as needed)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to run'
        required: true
        default: 'prod'
        type: choice
        options:
        - dev
        - staging
        - prod

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-1

jobs:
  check-infrastructure:
    name: 'Check Infrastructure Status'
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'prod' }}
    outputs:
      infrastructure-exists: ${{ steps.check.outputs.exists }}

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: GitHubActions-InfraCheck

    - name: Check Infrastructure
      id: check
      run: |
        echo "üîç Checking if infrastructure is deployed..."
        ENV_NAME="${{ github.event.inputs.environment || 'prod' }}"

        # Check if Step Functions state machine exists
        if aws stepfunctions list-state-machines --query "stateMachines[?contains(name, 'aws-ssm-fetcher-${ENV_NAME}')]" --output text | grep -q .; then
          echo "‚úÖ Infrastructure appears to be deployed for environment: $ENV_NAME"
          echo "exists=true" >> $GITHUB_OUTPUT
        else
          echo "‚ùå Infrastructure not found for environment: $ENV_NAME"
          echo "Please deploy infrastructure first using: gh workflow run 'Terraform Deployment' --ref main"
          echo "exists=false" >> $GITHUB_OUTPUT
        fi

  execute-pipeline:
    name: 'Execute AWS SSM Data Fetcher Pipeline'
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'prod' }}
    needs: check-infrastructure
    if: needs.check-infrastructure.outputs.infrastructure-exists == 'true'

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: GitHubActions-ScheduledExecution

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false

    - name: Get Infrastructure Outputs
      working-directory: ./terraform
      run: |
        # Initialize Terraform to access outputs
        echo "üîß Initializing Terraform to access state..."
        terraform init \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=aws-ssm-fetcher/${{ github.event.inputs.environment || 'prod' }}/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"

        # Check if state file exists and has resources
        echo "üîç Checking Terraform state..."
        if ! terraform state list >/dev/null 2>&1; then
          echo "‚ùå No Terraform state found. Please deploy infrastructure first."
          echo "To deploy: gh workflow run 'Terraform Deployment' --ref main"
          exit 1
        fi

        # Get infrastructure details with error handling
        echo "üìã Getting infrastructure outputs..."

        # Use a safer method to set environment variables, avoiding ANSI codes
        if STEP_FUNCTION_ARN=$(terraform output -raw step_function_arn 2>/dev/null | tr -d '\033' | sed 's/\[[0-9;]*[mGK]//g'); then
          if [ -n "$STEP_FUNCTION_ARN" ] && [ "$STEP_FUNCTION_ARN" != "null" ]; then
            {
              echo "STEP_FUNCTION_ARN<<EOF"
              echo "$STEP_FUNCTION_ARN"
              echo "EOF"
            } >> $GITHUB_ENV
            echo "‚úÖ Step Function ARN: $STEP_FUNCTION_ARN"
          else
            echo "‚ùå Step Function ARN is empty or null"
            exit 1
          fi
        else
          echo "‚ùå Could not get Step Function ARN from Terraform outputs"
          exit 1
        fi

        if S3_BUCKET=$(terraform output -raw s3_bucket_name 2>/dev/null | tr -d '\033' | sed 's/\[[0-9;]*[mGK]//g'); then
          if [ -n "$S3_BUCKET" ] && [ "$S3_BUCKET" != "null" ]; then
            {
              echo "S3_BUCKET<<EOF"
              echo "$S3_BUCKET"
              echo "EOF"
            } >> $GITHUB_ENV
            echo "‚úÖ S3 Bucket: $S3_BUCKET"
          else
            echo "‚ùå S3 Bucket name is empty or null"
            exit 1
          fi
        else
          echo "‚ùå Could not get S3 bucket name from Terraform outputs"
          exit 1
        fi

        if DASHBOARD_URL=$(terraform output -raw cloudwatch_dashboard_url 2>/dev/null | tr -d '\033' | sed 's/\[[0-9;]*[mGK]//g'); then
          if [ -n "$DASHBOARD_URL" ] && [ "$DASHBOARD_URL" != "null" ]; then
            {
              echo "DASHBOARD_URL<<EOF"
              echo "$DASHBOARD_URL"
              echo "EOF"
            } >> $GITHUB_ENV
            echo "‚úÖ Dashboard URL: $DASHBOARD_URL"
          else
            echo "‚ö†Ô∏è Dashboard URL is empty (this is optional)"
            {
              echo "DASHBOARD_URL<<EOF"
              echo ""
              echo "EOF"
            } >> $GITHUB_ENV
          fi
        else
          echo "‚ö†Ô∏è Could not get Dashboard URL (this is optional)"
          {
            echo "DASHBOARD_URL<<EOF"
            echo ""
            echo "EOF"
          } >> $GITHUB_ENV
        fi

    - name: Execute Step Function
      id: execute
      run: |
        echo "üöÄ Starting AWS SSM Data Fetcher pipeline execution..."

        # Validate required environment variables
        if [ -z "$STEP_FUNCTION_ARN" ]; then
          echo "‚ùå STEP_FUNCTION_ARN is not set"
          exit 1
        fi

        echo "üìç Using Step Function: $STEP_FUNCTION_ARN"

        # Validate Step Function exists
        echo "üîç Validating Step Function exists..."
        if ! aws stepfunctions describe-state-machine --state-machine-arn "$STEP_FUNCTION_ARN" >/dev/null 2>&1; then
          echo "‚ùå Step Function does not exist or is not accessible: $STEP_FUNCTION_ARN"
          echo "Please verify the infrastructure is deployed correctly."
          exit 1
        fi
        echo "‚úÖ Step Function validated successfully"

        # Create execution input with proper JSON formatting
        TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
        ENVIRONMENT="${{ github.event.inputs.environment || 'prod' }}"
        EXECUTION_ID="scheduled-$(date +%Y%m%d-%H%M%S)"

        # Create JSON input using jq for proper escaping
        EXECUTION_INPUT=$(jq -n \
          --arg source "github-actions-scheduled" \
          --arg timestamp "$TIMESTAMP" \
          --arg environment "$ENVIRONMENT" \
          --arg execution_id "$EXECUTION_ID" \
          '{
            source: $source,
            timestamp: $timestamp,
            environment: $environment,
            execution_id: $execution_id
          }')

        echo "üìã Execution input:"
        echo "$EXECUTION_INPUT" | jq '.'

        # Start execution with comprehensive error handling
        echo "‚ñ∂Ô∏è Starting Step Functions execution..."

        # Use temporary file for proper error capture
        TEMP_OUTPUT=$(mktemp)
        if aws stepfunctions start-execution \
          --state-machine-arn "$STEP_FUNCTION_ARN" \
          --name "$EXECUTION_ID" \
          --input "$EXECUTION_INPUT" \
          --output json > "$TEMP_OUTPUT" 2>&1; then

          EXECUTION_ARN=$(jq -r '.executionArn' "$TEMP_OUTPUT")
          echo "EXECUTION_ARN=$EXECUTION_ARN" >> $GITHUB_ENV
          echo "execution_arn=$EXECUTION_ARN" >> $GITHUB_OUTPUT
          echo "‚úÖ Execution started successfully: $EXECUTION_ARN"

          # Clean up temp file
          rm -f "$TEMP_OUTPUT"
        else
          echo "‚ùå Failed to start Step Functions execution"
          echo "Error details:"
          cat "$TEMP_OUTPUT"
          rm -f "$TEMP_OUTPUT"
          exit 1
        fi

    - name: Monitor Execution
      id: monitor
      run: |
        echo "‚è≥ Monitoring execution progress..."

        # Validate execution ARN
        if [ -z "$EXECUTION_ARN" ]; then
          echo "‚ùå EXECUTION_ARN is not set"
          exit 1
        fi

        echo "üìç Monitoring execution: $EXECUTION_ARN"

        # Wait for execution to complete (with timeout)
        MAX_WAIT=1800  # 30 minutes
        WAIT_TIME=0
        STATUS="RUNNING"
        RETRY_COUNT=0
        MAX_RETRIES=3

        # Initialize step summary
        {
          echo "## üìä Execution Progress"
          echo "- **Execution ARN**: \`$(basename "$EXECUTION_ARN")\`"
          echo "- **Status**: Initializing..."
          echo "- **Runtime**: 0s / ${MAX_WAIT}s"
          if [ -n "$DASHBOARD_URL" ]; then
            echo "- **Dashboard**: [View CloudWatch]($DASHBOARD_URL)"
          fi
          echo ""
          echo "**Started**: $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)"
        } > $GITHUB_STEP_SUMMARY

        while [ "$STATUS" = "RUNNING" ] && [ $WAIT_TIME -lt $MAX_WAIT ]; do
          sleep 60
          WAIT_TIME=$((WAIT_TIME + 60))

          # Get status with error handling and retries
          TEMP_STATUS_FILE=$(mktemp)
          if aws stepfunctions describe-execution \
            --execution-arn "$EXECUTION_ARN" \
            --output json > "$TEMP_STATUS_FILE" 2>&1; then

            STATUS=$(jq -r '.status' "$TEMP_STATUS_FILE" 2>/dev/null || echo "UNKNOWN")
            RETRY_COUNT=0

            echo "Status after ${WAIT_TIME}s: $STATUS"

            # Update step summary with progress
            {
              echo "## üìä Execution Progress"
              echo "- **Execution ARN**: \`$(basename "$EXECUTION_ARN")\`"
              echo "- **Status**: $STATUS"
              echo "- **Runtime**: ${WAIT_TIME}s / ${MAX_WAIT}s"
              if [ -n "$DASHBOARD_URL" ]; then
                echo "- **Dashboard**: [View CloudWatch]($DASHBOARD_URL)"
              fi
              echo ""
              echo "**Last Updated**: $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)"

              # Add progress bar
              PROGRESS=$((WAIT_TIME * 100 / MAX_WAIT))
              echo ""
              echo "**Progress**: ${PROGRESS}% (${WAIT_TIME}/${MAX_WAIT}s)"
            } > $GITHUB_STEP_SUMMARY

          else
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "‚ö†Ô∏è Could not get execution status (attempt $RETRY_COUNT/$MAX_RETRIES)"

            if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
              echo "‚ùå Failed to get execution status after $MAX_RETRIES retries"
              echo "Error details:"
              cat "$TEMP_STATUS_FILE"
              rm -f "$TEMP_STATUS_FILE"
              exit 1
            fi
          fi

          rm -f "$TEMP_STATUS_FILE"
        done

        # Handle timeout
        if [ $WAIT_TIME -ge $MAX_WAIT ] && [ "$STATUS" = "RUNNING" ]; then
          echo "‚è∞ Execution timed out after ${MAX_WAIT}s"
          STATUS="TIMED_OUT"
        fi

        echo "status=$STATUS" >> $GITHUB_OUTPUT
        echo "execution_duration=$WAIT_TIME" >> $GITHUB_OUTPUT

        # Get final execution details
        echo "üìã Getting final execution details..."
        TEMP_DETAILS_FILE=$(mktemp)
        if aws stepfunctions describe-execution \
          --execution-arn "$EXECUTION_ARN" \
          --output json > "$TEMP_DETAILS_FILE" 2>&1; then

          echo "Final status: $STATUS"
          echo "Execution details:"
          cat "$TEMP_DETAILS_FILE" | jq '.'
        else
          echo "‚ö†Ô∏è Could not retrieve final execution details"
          cat "$TEMP_DETAILS_FILE"
        fi
        rm -f "$TEMP_DETAILS_FILE"

    - name: Check Results
      if: steps.monitor.outputs.status == 'SUCCEEDED'
      run: |
        echo "‚úÖ Pipeline execution completed successfully!"

        # List generated reports
        echo "üìÑ Generated reports:"
        aws s3 ls s3://$S3_BUCKET/reports/ --recursive --human-readable

        # Get latest reports info
        LATEST_REPORTS=$(aws s3 ls s3://$S3_BUCKET/reports/ --recursive | tail -5)

        echo "## üéâ Execution Completed Successfully" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìÑ Generated Reports" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "```" >> $GITHUB_STEP_SUMMARY
        echo "$LATEST_REPORTS" >> $GITHUB_STEP_SUMMARY
        echo "```" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìä Monitoring" >> $GITHUB_STEP_SUMMARY
        echo "- [CloudWatch Dashboard]($DASHBOARD_URL)" >> $GITHUB_STEP_SUMMARY
        echo "- S3 Bucket: \`$S3_BUCKET\`" >> $GITHUB_STEP_SUMMARY

    - name: Handle Failure
      if: steps.monitor.outputs.status != 'SUCCEEDED'
      run: |
        FINAL_STATUS="${{ steps.monitor.outputs.status }}"
        EXECUTION_DURATION="${{ steps.monitor.outputs.execution_duration }}"

        echo "‚ùå Pipeline execution failed with status: $FINAL_STATUS"
        echo "‚è±Ô∏è Execution duration: ${EXECUTION_DURATION}s"

        # Get comprehensive failure details
        echo "üîç Gathering failure diagnostics..."

        TEMP_EXEC_FILE=$(mktemp)
        TEMP_HISTORY_FILE=$(mktemp)

        # Get execution details
        if aws stepfunctions describe-execution \
          --execution-arn "$EXECUTION_ARN" \
          --output json > "$TEMP_EXEC_FILE" 2>&1; then

          CAUSE=$(jq -r '.cause // "No cause available"' "$TEMP_EXEC_FILE")
          START_DATE=$(jq -r '.startDate' "$TEMP_EXEC_FILE")
          STOP_DATE=$(jq -r '.stopDate // "Still running"' "$TEMP_EXEC_FILE")

          echo "üìã Execution Details:"
          echo "  Start: $START_DATE"
          echo "  Stop: $STOP_DATE"
          echo "  Cause: $CAUSE"
        else
          echo "‚ö†Ô∏è Could not retrieve execution details"
          CAUSE="Could not retrieve failure details"
        fi

        # Get execution history for more detailed diagnostics
        if aws stepfunctions get-execution-history \
          --execution-arn "$EXECUTION_ARN" \
          --max-items 10 \
          --reverse-order \
          --output json > "$TEMP_HISTORY_FILE" 2>&1; then

          echo "üìú Recent execution events:"
          jq -r '.events[] | "\(.timestamp) [\(.type)]: \(.stateEnteredEventDetails.name // .stateExitedEventDetails.name // .taskFailedEventDetails.resourceType // "N/A")"' "$TEMP_HISTORY_FILE" | head -10
        else
          echo "‚ö†Ô∏è Could not retrieve execution history"
        fi

        # Create comprehensive failure summary
        {
          echo "## ‚ùå Execution Failed"
          echo ""
          echo "**Final Status**: $FINAL_STATUS"
          echo "**Duration**: ${EXECUTION_DURATION}s"
          echo "**Execution ARN**: \`$(basename "$EXECUTION_ARN")\`"
          echo ""

          if [ "$FINAL_STATUS" = "TIMED_OUT" ]; then
            echo "**Issue**: Execution exceeded the 30-minute timeout limit"
            echo ""
            echo "**Possible Causes**:"
            echo "- Large dataset processing taking longer than expected"
            echo "- Network timeouts or API throttling"
            echo "- Lambda function cold starts or memory issues"
            echo ""
          else
            echo "**Failure Details**:"
            echo "\`\`\`"
            echo "$CAUSE"
            echo "\`\`\`"
            echo ""
          fi

          echo "**Troubleshooting Steps**:"
          echo "1. üîç [View CloudWatch Dashboard]($DASHBOARD_URL)"
          echo "2. üìã Check Step Functions execution in AWS Console"
          echo "3. üìù Review Lambda function logs in CloudWatch"
          echo "4. üîÑ Try running the pipeline again"
          echo ""
          echo "**AWS Console Links**:"
          echo "- [Step Functions Console](https://console.aws.amazon.com/states/home?region=${{ env.AWS_REGION }}#/executions/details/$(echo $EXECUTION_ARN | sed 's/:/\%3A/g'))"
          echo "- [CloudWatch Logs](https://console.aws.amazon.com/cloudwatch/home?region=${{ env.AWS_REGION }}#logsV2:log-groups)"
          echo ""
          echo "**Execution Time**: $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)"
        } > $GITHUB_STEP_SUMMARY

        # Clean up temp files
        rm -f "$TEMP_EXEC_FILE" "$TEMP_HISTORY_FILE"

        exit 1

    - name: Cleanup Old Reports
      if: steps.monitor.outputs.status == 'SUCCEEDED'
      run: |
        echo "üßπ Cleaning up old reports..."

        # Keep only the last 30 days of reports
        CUTOFF_DATE=$(date -d "30 days ago" +%Y-%m-%d 2>/dev/null || date -v-30d +%Y-%m-%d)

        # This is a simple cleanup - in production you might want more sophisticated logic
        echo "Keeping reports newer than $CUTOFF_DATE"

        # The S3 lifecycle policies in Terraform handle this automatically,
        # but this provides manual cleanup if needed
        echo "‚úÖ Cleanup completed (handled by S3 lifecycle policies)"

  notify-results:
    needs: [check-infrastructure, execute-pipeline]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Notify Success
      if: needs.execute-pipeline.result == 'success'
      run: |
        echo "‚úÖ Scheduled execution completed successfully"
        # Add Slack/email notification here if needed

    - name: Notify Infrastructure Missing
      if: needs.check-infrastructure.outputs.infrastructure-exists == 'false'
      run: |
        echo "‚ÑπÔ∏è Scheduled execution skipped - infrastructure not deployed"
        echo "This is expected behavior when AWS infrastructure hasn't been deployed yet."
        echo "To deploy infrastructure: gh workflow run 'Terraform Deployment' --ref main"
        # Add informational notification here if needed

    - name: Notify Failure
      if: needs.execute-pipeline.result == 'failure'
      run: |
        echo "‚ùå Scheduled execution failed"
        # Add Slack/email notification here if needed
        exit 1
